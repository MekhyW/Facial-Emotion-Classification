{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b20920",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ed95f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_mesh_connections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FACEMESH_TESSELATION\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras import Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Flatten, BatchNormalization, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2, l1, l1_l2\n",
    "\n",
    "# from spektral.layers import GraphConv\n",
    "# GRaphConv is deprecated, use GCNConv or GCSConv instead\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.utils import normalized_laplacian\n",
    "from spektral.layers import GCSConv  # as GraphConv\n",
    "from spektral.layers import GINConv # as GraphConv\n",
    "from spektral.layers import GCNConv  # as GraphConv\n",
    "\n",
    "from spektral.utils.convolution import gcn_filter  # For GCNConv\n",
    "from spektral.utils.convolution import normalized_adjacency  # For GCSConv\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mediapipe.python.solutions.face_mesh_connections import FACEMESH_TESSELATION\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5e54a",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c560c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "l2_reg = 5e-4\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "es_patience = 200\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c861d",
   "metadata": {},
   "source": [
    "Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e67f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mediapipe_adjacency_matrix():\n",
    "    \"\"\"Create adjacency matrix from MediaPipe face mesh connections\"\"\"\n",
    "    # MediaPipe face mesh has 468 landmarks\n",
    "    n_nodes = 468\n",
    "    adj_matrix = np.zeros((n_nodes, n_nodes), dtype=np.float32)\n",
    "    \n",
    "    # Add edges based on MediaPipe face mesh connections\n",
    "    for connection in FACEMESH_TESSELATION:\n",
    "        i, j = connection[0], connection[1]\n",
    "        if i < n_nodes and j < n_nodes:  # Ensure indices are valid\n",
    "            adj_matrix[i, j] = 1.0\n",
    "            adj_matrix[j, i] = 1.0  # Undirected graph\n",
    "    \n",
    "    # Add self-loops\n",
    "    np.fill_diagonal(adj_matrix, 1.0)\n",
    "    \n",
    "    return adj_matrix\n",
    "\n",
    "def normalize_mesh_points(mesh_points):\n",
    "    \"\"\"Normalize mesh points to unit scale\"\"\"\n",
    "    mesh_points = np.array(mesh_points, dtype=np.float32)\n",
    "    \n",
    "    # Center the points\n",
    "    center = np.mean(mesh_points, axis=0)\n",
    "    mesh_points = mesh_points - center\n",
    "    \n",
    "    # Scale to unit variance\n",
    "    scale = np.std(mesh_points)\n",
    "    if scale > 0:\n",
    "        mesh_points = mesh_points / scale\n",
    "    \n",
    "    return mesh_points\n",
    "\n",
    "def load_mesh_data(path_list, limit=1.0):\n",
    "    \"\"\"Load and preprocess mesh data from directories\"\"\"\n",
    "    all_meshes = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for emotion_idx, path in enumerate(path_list):\n",
    "        if not path.exists():\n",
    "            print(f\"Path does not exist: {path}\")\n",
    "            continue\n",
    "            \n",
    "        files = os.listdir(path)\n",
    "        num_files = len(files)\n",
    "        files_to_process = int(num_files * limit)\n",
    "        \n",
    "        emotion_meshes = []\n",
    "        processed = 0\n",
    "        \n",
    "        for file in files[:files_to_process]:\n",
    "            if not file.endswith('.json'):\n",
    "                continue\n",
    "                \n",
    "            file_path = path / file\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                # Ensure we have exactly 468 landmarks\n",
    "                if len(data) != 468:\n",
    "                    print(f\"Skipping {file}: expected 468 landmarks, got {len(data)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Normalize the mesh points\n",
    "                normalized_mesh = normalize_mesh_points(data)\n",
    "                emotion_meshes.append(normalized_mesh)\n",
    "                processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Loaded {processed} samples for emotion {emotion_idx}\")\n",
    "        \n",
    "        # Add to overall lists\n",
    "        all_meshes.extend(emotion_meshes)\n",
    "        all_labels.extend([emotion_idx] * len(emotion_meshes))\n",
    "    \n",
    "    return np.array(all_meshes, dtype=np.float32), np.array(all_labels, dtype=np.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc05d1",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "current_path = pathlib.Path().absolute()\n",
    "parent_path = current_path.parent\n",
    "\n",
    "path_list = [\n",
    "    parent_path / 'angry_meshpoints',\n",
    "    parent_path / 'disgusted_meshpoints', \n",
    "    parent_path / 'happy_meshpoints',\n",
    "    parent_path / 'neutral_meshpoints',\n",
    "    parent_path / 'sad_meshpoints',\n",
    "    parent_path / 'surprised_meshpoints'\n",
    "]\n",
    "\n",
    "emotion_names = ['angry', 'disgusted', 'happy', 'neutral', 'sad', 'surprised']\n",
    "\n",
    "print(\"Loading mesh data...\")\n",
    "X_data, y_data = load_mesh_data(path_list, limit=1.0)\n",
    "\n",
    "print(f\"Total samples loaded: {len(X_data)}\")\n",
    "print(f\"Data shape: {X_data.shape}\")\n",
    "print(f\"Labels shape: {y_data.shape}\")\n",
    "\n",
    "# Print class distribution\n",
    "unique, counts = np.unique(y_data, return_counts=True)\n",
    "for emotion_idx, count in zip(unique, counts):\n",
    "    print(f\"{emotion_names[emotion_idx]}: {count} samples\")\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, shuffle=True, random_state=RANDOM_SEED, stratify=y_data\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, shuffle=True, random_state=RANDOM_SEED, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "X_train_tensor = tf.constant(X_train, dtype=tf.float32)\n",
    "y_train_tensor = tf.constant(y_train, dtype=tf.int32)\n",
    "X_val_tensor = tf.constant(X_val, dtype=tf.float32)\n",
    "y_val_tensor = tf.constant(y_val, dtype=tf.int32)\n",
    "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
    "y_test_tensor = tf.constant(y_test, dtype=tf.int32)\n",
    "\n",
    "print(f\"Train tensor shape: {X_train_tensor.shape}\")\n",
    "print(f\"Train labels shape: {y_train_tensor.shape}\")\n",
    "\n",
    "# Model parameters\n",
    "n_out = 6  # Number of emotion classes\n",
    "N = X_train_tensor.shape[1]  # Number of nodes (468)\n",
    "F = X_train_tensor.shape[2]  # Feature dimensionality (2 for x,y coordinates)\n",
    "\n",
    "print(f\"Number of nodes: {N}\")\n",
    "print(f\"Number of features per node: {F}\")\n",
    "\n",
    "# Create adjacency matrix\n",
    "print(\"Creating adjacency matrix...\")\n",
    "adj_matrix = get_mediapipe_adjacency_matrix()\n",
    "print(f\"Adjacency matrix shape: {adj_matrix.shape}\")\n",
    "print(f\"Number of edges: {np.sum(adj_matrix > 0) // 2}\")  # Divide by 2 for undirected graph\n",
    "\n",
    "# Convert to sparse tensor for efficiency\n",
    "adj_sparse = scipy.sparse.csr_matrix(adj_matrix)\n",
    "adj_tensor = sp_matrix_to_sp_tensor(adj_sparse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142bd509",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48168ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the GNN model\n",
    "print(\"Building model...\")\n",
    "X_in = Input(shape=(N, F), name='node_features')\n",
    "\n",
    "# First GNN layer\n",
    "x = GINConv(64, activation=\"relu\", kernel_regularizer=l2(l2_reg))([X_in, adj_tensor])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Second GNN layer\n",
    "x = GINConv(32, activation=\"relu\", kernel_regularizer=l2(l2_reg))([x, adj_tensor])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Global pooling (flatten all node features)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully connected layers\n",
    "x = Dense(256, activation=\"relu\", kernel_regularizer=l2(l2_reg))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, activation=\"relu\", kernel_regularizer=l2(l2_reg))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(n_out, activation=\"softmax\", name='emotion_output')(x)\n",
    "\n",
    "# Create and compile model\n",
    "model = Model(inputs=X_in, outputs=output)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073dee35",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec66621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val_tensor, y_val_tensor),\n",
    "    epochs=epochs*5,\n",
    "    callbacks=[EarlyStopping(patience=es_patience, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Training completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

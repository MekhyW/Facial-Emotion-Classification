{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27504b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from spektral.layers import GINConv\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute adjacency and sparse tensor\n",
    "\n",
    "def get_mediapipe_adj_tensor():\n",
    "    adj = get_mediapipe_adjacency_matrix()  # your existing function\n",
    "    adj_sp = scipy.sparse.csr_matrix(adj)\n",
    "    return sp_matrix_to_sp_tensor(adj_sp)\n",
    "\n",
    "# Build function\n",
    "def build_gnn_face_emotion_model(\n",
    "    N: int,\n",
    "    F: int,\n",
    "    n_out: int,\n",
    "    l2_reg: float = 5e-4,\n",
    "    learning_rate: float = 1e-3\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Builds and compiles the GNN face emotion classification model.\n",
    "\n",
    "    Args:\n",
    "        N: Number of nodes (e.g., 468).\n",
    "        F: Feature dimensionality per node (e.g., 2 for x, y).\n",
    "        n_out: Number of emotion classes.\n",
    "        l2_reg: L2 regularization factor.\n",
    "        learning_rate: Learning rate for the Adam optimizer.\n",
    "\n",
    "    Returns:\n",
    "        A compiled tf.keras.Model instance.\n",
    "    \"\"\"\n",
    "    # Prepare adjacency as sparse tensor\n",
    "    adj_tensor = get_mediapipe_adj_tensor()\n",
    "\n",
    "    # Input for node features\n",
    "    X_in = Input(shape=(N, F), name='node_features')\n",
    "\n",
    "    # First GNN block\n",
    "    x = GINConv(64, activation='relu', kernel_regularizer=l2(l2_reg))([X_in, adj_tensor])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Second GNN block\n",
    "    x = GINConv(32, activation='relu', kernel_regularizer=l2(l2_reg))([x, adj_tensor])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Global pooling (flatten)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Dense layers\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output\n",
    "    output = Dense(n_out, activation='softmax', name='emotion_output')(x)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=X_in, outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# model = build_gnn_face_emotion_model(N=468, F=2, n_out=6)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)define your `build_model()` exactly as you did originally,\n",
    "# including the `get_mediapipe_adjacency_matrix() + sp_matrix_to_sp_tensor` call.\n",
    "model = build_gnn_face_emotion_model(N=468, F=2, n_out=6)  \n",
    "model.load_weights(\"gnn_face_emotion2.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfcb3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from spektral.layers import GINConv\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "import scipy.sparse\n",
    "from mediapipe.python.solutions.face_mesh_connections import FACEMESH_TESSELATION\n",
    "import time\n",
    "\n",
    "def get_mediapipe_adjacency_matrix():\n",
    "    \"\"\"Create adjacency matrix from MediaPipe face mesh connections\"\"\"\n",
    "    # MediaPipe face mesh has 468 landmarks (without iris when refine_landmarks=False)\n",
    "    n_nodes = 468\n",
    "    adj_matrix = np.zeros((n_nodes, n_nodes), dtype=np.float32)\n",
    "    \n",
    "    # Add edges based on MediaPipe face mesh connections\n",
    "    for connection in FACEMESH_TESSELATION:\n",
    "        i, j = connection[0], connection[1]\n",
    "        if i < n_nodes and j < n_nodes:  # Ensure indices are valid for 468 landmarks\n",
    "            adj_matrix[i, j] = 1.0\n",
    "            adj_matrix[j, i] = 1.0  # Undirected graph\n",
    "    \n",
    "    # Add self-loops\n",
    "    np.fill_diagonal(adj_matrix, 1.0)\n",
    "    \n",
    "    return adj_matrix\n",
    "\n",
    "# --- Constants and Initializations ---\n",
    "# Define the emotion labels in the same order as your training data\n",
    "EMOTIONS = [\"Angry\", \"Disgusted\", \"Happy\", \"Neutral\", \"Sad\", \"Surprised\"]\n",
    "\n",
    "# Emotion colors for display (BGR format for OpenCV)\n",
    "EMOTION_COLORS = {\n",
    "    \"Angry\": (0, 0, 255),      # Red\n",
    "    \"Disgusted\": (0, 128, 0),  # Green\n",
    "    \"Happy\": (0, 255, 255),    # Yellow\n",
    "    \"Neutral\": (128, 128, 128), # Gray\n",
    "    \"Sad\": (255, 0, 0),        # Blue\n",
    "    \"Surprised\": (0, 165, 255) # Orange\n",
    "}\n",
    "\n",
    "# Load the trained model with improved method\n",
    "try:\n",
    "    print(\"Loading trained GNN model...\")\n",
    "    from model_utils import load_model_with_validation\n",
    "    model, metadata, adj_tensor_loaded = load_model_with_validation(\"emotion_gnn_model.h5\")\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"❌ Could not load model. Make sure you've trained the model first.\")\n",
    "        exit()\n",
    "        \n",
    "    print(\"✅ Model loaded successfully.\")\n",
    "    print(f\"Model input shape: {model.input_shape}\")\n",
    "    print(f\"Model output shape: {model.output_shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"Make sure 'emotion_gnn_model.h5' exists and was trained with the GNN code.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1, \n",
    "    refine_landmarks=False,  # Use 468 landmarks (no iris)\n",
    "    min_detection_confidence=0.7, \n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Create adjacency matrix for the model\n",
    "print(\"Creating adjacency matrix...\")\n",
    "adj_matrix = get_mediapipe_adjacency_matrix()\n",
    "adj_sparse = scipy.sparse.csr_matrix(adj_matrix)\n",
    "adj_tensor = sp_matrix_to_sp_tensor(adj_sparse)\n",
    "\n",
    "print(f\"Adjacency matrix shape: {adj_matrix.shape}\")\n",
    "print(f\"Number of edges: {np.sum(adj_matrix > 0) // 2}\")\n",
    "\n",
    "# Prediction smoothing\n",
    "prediction_history = []\n",
    "history_size = 5  # Number of frames to average for smoothing\n",
    "\n",
    "def normalize_mesh_points(mesh_points):\n",
    "    \"\"\"Normalize mesh points to match training data preprocessing\"\"\"\n",
    "    mesh_points = np.array(mesh_points, dtype=np.float32)\n",
    "    \n",
    "    # Center the points\n",
    "    center = np.mean(mesh_points, axis=0)\n",
    "    mesh_points = mesh_points - center\n",
    "    \n",
    "    # Scale to unit variance\n",
    "    scale = np.std(mesh_points)\n",
    "    if scale > 0:\n",
    "        mesh_points = mesh_points / scale\n",
    "    \n",
    "    return mesh_points\n",
    "\n",
    "def draw_prediction_bars(frame, predictions, current_emotion):\n",
    "    \"\"\"Draw probability bars for all emotions\"\"\"\n",
    "    bar_width = 200\n",
    "    bar_height = 25\n",
    "    start_x = 10\n",
    "    start_y = 80\n",
    "    \n",
    "    for i, (emotion, prob) in enumerate(zip(EMOTIONS, predictions[0])):\n",
    "        y_pos = start_y + i * (bar_height + 5)\n",
    "        \n",
    "        # Background bar\n",
    "        cv2.rectangle(frame, (start_x, y_pos), (start_x + bar_width, y_pos + bar_height), \n",
    "                     (50, 50, 50), -1)\n",
    "        \n",
    "        # Probability bar\n",
    "        fill_width = int(bar_width * prob)\n",
    "        color = EMOTION_COLORS[emotion] if emotion == current_emotion else (100, 100, 100)\n",
    "        cv2.rectangle(frame, (start_x, y_pos), (start_x + fill_width, y_pos + bar_height), \n",
    "                     color, -1)\n",
    "        \n",
    "        # Text label\n",
    "        cv2.putText(frame, f\"{emotion}: {prob:.1%}\", (start_x + bar_width + 10, y_pos + 18), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "# --- Main Webcam Loop ---\n",
    "print(\"Starting webcam...\")\n",
    "print(\"Press 'q' to quit, 's' to save current frame, 'r' to reset prediction history\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Set camera properties for better performance\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "frame_count = 0\n",
    "fps_start_time = time.time()\n",
    "fps = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    H, W, _ = frame.shape\n",
    "    \n",
    "    # Calculate FPS\n",
    "    if frame_count % 30 == 0:\n",
    "        fps_end_time = time.time()\n",
    "        fps = 30 / (fps_end_time - fps_start_time)\n",
    "        fps_start_time = fps_end_time\n",
    "\n",
    "    # Flip frame horizontally for mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the frame with MediaPipe\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        # Use the landmarks of the first detected face\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        \n",
    "        # --- Extract and preprocess landmarks for the model ---\n",
    "        # 1. Extract X, Y coordinates\n",
    "        mesh_points = np.array(\n",
    "            [[p.x * W, p.y * H] for p in face_landmarks.landmark],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        print(f\"Number of landmarks detected: {len(mesh_points)}\")  # Should be 468\n",
    "        \n",
    "        # 2. Normalize the landmarks (same as training)\n",
    "        normalized_points = normalize_mesh_points(mesh_points)\n",
    "        \n",
    "        # 3. Reshape for the model input (add batch dimension)\n",
    "        model_input = np.expand_dims(normalized_points, axis=0)\n",
    "        \n",
    "        try:\n",
    "            # --- Predict Emotion ---\n",
    "            prediction = model.predict(model_input, verbose=0)\n",
    "            \n",
    "            # Add to prediction history for smoothing\n",
    "            prediction_history.append(prediction[0])\n",
    "            if len(prediction_history) > history_size:\n",
    "                prediction_history.pop(0)\n",
    "            \n",
    "            # Average predictions for smoother results\n",
    "            smoothed_prediction = np.mean(prediction_history, axis=0)\n",
    "            emotion_index = np.argmax(smoothed_prediction)\n",
    "            emotion_label = EMOTIONS[emotion_index]\n",
    "            confidence = smoothed_prediction[emotion_index]\n",
    "            \n",
    "            # --- Display the result on the frame ---\n",
    "            # Draw face bounding box\n",
    "            x_min = int(np.min(mesh_points[:, 0])) - 20\n",
    "            y_min = int(np.min(mesh_points[:, 1])) - 20\n",
    "            x_max = int(np.max(mesh_points[:, 0])) + 20\n",
    "            y_max = int(np.max(mesh_points[:, 1])) + 20\n",
    "            \n",
    "            # Draw bounding box with emotion color\n",
    "            color = EMOTION_COLORS[emotion_label]\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, 3)\n",
    "            \n",
    "            # Display the emotion label and confidence\n",
    "            text = f\"{emotion_label}: {confidence:.1%}\"\n",
    "            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]\n",
    "            \n",
    "            # Background for text\n",
    "            cv2.rectangle(frame, (x_min, y_min - 40), \n",
    "                         (x_min + text_size[0] + 10, y_min), color, -1)\n",
    "            \n",
    "            # Text\n",
    "            cv2.putText(frame, text, (x_min + 5, y_min - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Draw face mesh (optional - can be disabled for better performance)\n",
    "            # mp_drawing.draw_landmarks(\n",
    "            #     frame, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            #     landmark_drawing_spec=None,\n",
    "            #     connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "            #         color=(0, 255, 0), thickness=1, circle_radius=1)\n",
    "            # )\n",
    "            \n",
    "            # Draw prediction bars\n",
    "            draw_prediction_bars(frame, [smoothed_prediction], emotion_label)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            cv2.putText(frame, \"Prediction Error\", (20, 40), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"No Face Detected\", (20, 40), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        # Clear prediction history when no face is detected\n",
    "        prediction_history.clear()\n",
    "\n",
    "    # Display FPS\n",
    "    cv2.putText(frame, f\"FPS: {fps:.1f}\", (W - 120, 30), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    # Display instructions\n",
    "    cv2.putText(frame, \"Press 'q' to quit, 'r' to reset\", (10, H - 20), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    # Show the final frame\n",
    "    cv2.imshow('Facial Emotion Recognition (GNN)', frame)\n",
    "\n",
    "    # Handle key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('r'):\n",
    "        prediction_history.clear()\n",
    "        print(\"Prediction history reset\")\n",
    "    elif key == ord('s'):\n",
    "        filename = f\"emotion_frame_{int(time.time())}.jpg\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        print(f\"Frame saved as {filename}\")\n",
    "\n",
    "print(\"Shutting down...\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "cap.release()\n",
    "cv2.waitKey(1) \n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "face_mesh.close()\n",
    "\n",
    "print(\"✅ Application closed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
